\documentclass{article}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\blambda}{\mbox{\boldmath $\lambda$}}
\author{Anastasios Panagiotelis}
\title{Notes on the Logistic Regression Model}
\date{\today}
\begin{document}
\maketitle
The Logistic Regression model, is a regression model for a {\bf binary dependent variable}.  The dependent variable is either $0$ or $1$.  For a binary dependent variable, the expected value $E[Y]=\mbox{Pr}(Y=1)$.  This result follows from the definition of an expected value for a discrete variable.
\begin{equation}
E[Y]=\sum_{D(Y)}y\mbox{Pr}(Y=y)
\end{equation}
where $D(Y)$ is the domain of $Y$ which for a binary variable is $\left\{0,1\right\}$.  Therefore,
\begin{align}
E[Y]&=\sum_{y=0,1}y\mbox{Pr}(Y=y)\\
E[Y]&=0\times\mbox{Pr}(Y=0)+1\times\mbox{Pr}(Y=1)\\
E[Y]&=\mbox{Pr}(Y=1)
\end{align}
Suppose that for the $i^{th}$ observation, $E\left[Y_i\right]$ depends on a linear combination of $p$ covariates ${\bm x}_i'\bbeta$, where,
\begin{equation}
{\bm x}_i=\left(
\begin{array}{c}
x_{i1}\\
x_{i2}\\
\vdots\\
x_{ip}
\end{array}
\right)
\quad\quad
{\bbeta}_i=\left(
\begin{array}{c}
\beta_{1}\\
\beta_{2}\\
\vdots\\
\beta_{p}
\end{array}
\right)
\end{equation}
For example
\begin{align*}
y_i& \quad\quad\mbox{equals 1 if the person $i$ owns a house and 0 if they don't own a house}\\
x_{i1}& \quad\quad\mbox{is the average house price in the city where person $i$ lives}\\
x_{i2}& \quad\quad\mbox{is the salary of person $i$}\\
x_{i3}& \quad\quad\mbox{is the age of person $i$}\\
\end{align*}   
We do {\bf not} use a {\bf linear regression model} where $E\left[Y_i|{\bm X_i}={\bm x_i}\right]={\bm x}_i'\bbeta$.  Note that ${\bm x}_i'\bbeta$ can be anything from $-\infty$ to $\infty$. This is not a problem for continuous $Y$, but for binary $Y$, $E\left[Y_i|{\bm X_i}={\bm x_i}\right]=\Pr(Y_i=1|{\bm X_i}={\bm x_i})$. A probability must lie between $0$ and $1$.
\section*{Logistic Regression}
To overcome this problem, Logistic regression uses the {\bf Logistic} link function:
\begin{equation}
\Lambda(z)=\frac{e^z}{e^z+1}
\end{equation}
For any value $-\infty\leq z\leq \infty$, $0\leq \Lambda(z)\leq 1$. In a picture:
\begin{center}
\includegraphics[height=8cm]{logitf.pdf}
\end{center}
The logistic regression model is defined as:
\begin{equation}
E\left[Y_i|{\bm X_i}={\bm x_i}\right]=\mbox{Pr}\left(Y_i=1|{\bm X_i}={\bm x_i}\right)=\Lambda\left({{\bm x'_i}\bbeta}\right)
\end{equation}
The probability mass function is given by
\begin{equation}
\mbox{Pr}(Y_i=y_i|{\bm X_i}={\bm x_i})=\Lambda({{\bm x'_i}\bbeta})^{y_i}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-y_i)}
\end{equation}
Note:
\begin{align*}
\mbox{Pr}(Y_i=1|{\bm X_i}={\bm x_i})&=\Lambda({{\bm x'_i}\bbeta})^{1}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-1)}\\
&=\Lambda({{\bm x'_i}\bbeta}))\\
\mbox{Pr}(Y_i=0|{\bm X_i}={\bm x_i})&=\Lambda({{\bm x'_i}\bbeta})^{0}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-0)}\\
&=1-\Lambda({{\bm x'_i}\bbeta}))\\
\end{align*}
The Likelihood for a sample $\left(y_i,{\bm x_i}\right)$ for $i=1,2,\ldots,n$ is
\begin{equation}
L(\bbeta)=\prod_{i=1}^n \Lambda({{\bm x'_i}\bbeta})^{y_i}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-y_i)} 
\end{equation}
\section*{Log Likelihood, Gradient and Hessian}
\subsection*{Log Likelihood}
The log likelihood is given by
\begin{align*}
lnL(\bbeta)&=ln\left(\prod_{i=1}^n \Lambda({{\bm x'_i}\bbeta})^{y_i}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-y_i)}\right)\\
&=\sum_{i=1}^nln\left( \Lambda({{\bm x'_i}\bbeta})^{y_i}\left(1-\Lambda({{\bm x'_i}\bbeta})\right)^{(1-y_i)}\right)\\
&=\sum_{i=1}^n\left[ y_i ln\left( \Lambda({{\bm x'_i}\bbeta})\right)+(1-y_i)ln\left(1-\Lambda({{\bm x'_i}\bbeta})\right)\right]\\
&=\sum_{i=1}^n\left[ y_i ln\left(\frac{e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}\right)+(1-y_i)ln\left(
1-\frac{e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}
\right)\right]\\
&=\sum_{i=1}^n\left[ y_i ln\left(\frac{e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}\right)+(1-y_i)ln\left(
\frac{1+e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}-\frac{e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}
\right)\right]\\
&=\sum_{i=1}^n\left[ y_i ln\left(\frac{e^{{\bm x'_i}\bbeta}}{1+e^{{\bm x'_i}\bbeta}}\right)+(1-y_i)ln\left(
\frac{1}{1+e^{{\bm x'_i}\bbeta}}
\right)\right]\\
&=\sum_{i=1}^n\left[
y_i\left(
ln\left(e^{{\bm x'_i}\bbeta}\right)
-ln\left(1+e^{{\bm x'_i}\bbeta}\right)
\right)
-(1-y_i)\left(
ln\left(1+e^{{\bm x'_i}\bbeta}\right)
\right)
\right]\\
&=\sum_{i=1}^n\left[
y_i{{\bm x'_i}\bbeta}
-y_iln\left(1+e^{{\bm x'_i}\bbeta}\right)
-ln\left(1+e^{{\bm x'_i}\bbeta}\right)
+y_iln\left(1+e^{{\bm x'_i}\bbeta}\right)
\right]\\
&=\sum_{i=1}^n\left[
y_i{{\bm x'_i}\bbeta}
-ln\left(1+e^{{\bm x'_i}\bbeta}\right)
\right]\\
&=\sum_{i=1}^n\left(
y_i{{{\bm x'_i}\bbeta}}
\right)
-\sum_{i=1}^n\left[
ln\left(1+e^{{{\bm x'_i}\bbeta}}\right)
\right]\\
&={\bm y}'X\bbeta
-\sum_{i=1}^n\left[
ln\left(1+e^{{{\bm x'_i}\bbeta}}\right)
\right]\,,
\end{align*}
where,
\begin{equation}
{\bm y}=\left(
\begin{array}{c}
y_1\\
y_1\\
\vdots\\
y_n
\end{array}
\right)
\quad\mbox{and}\quad
X=\left(
\begin{array}{c}
{\bm x'_1}\\
{\bm x'_2}\\
\vdots\\
{\bm x'_n}\\
\end{array}
\right)
\end{equation}
\subsection*{Gradient}
Need to find
\begin{equation}
\frac{\partial lnL(\bbeta)}{\partial\bbeta}=\frac{\partial}{\partial\bbeta}\left({\bm y}'X\bbeta
-\sum_{i=1}^n\left[
ln\left(1+e^{{{\bm x'_i}\bbeta}}\right)
\right]\right)
\end{equation}
Break this into smaller pieces. Since ${\bm y}'X\bbeta$ is a scalar, ${\bm y}'X\bbeta=\bbeta' X'{\bm y}$
\begin{equation}
\frac{\partial}{\partial\bbeta}\left({\bm y}'X\bbeta\right)=\frac{\partial}{\partial\bbeta}\left(\bbeta X'{\bm y}\right)=X'{\bm y}
\end{equation}
Next piece $\frac{\partial}{\partial\bbeta}\left(ln\left(1+e^{{{\bm x'_i}\bbeta}}\right)\right)=
\frac{\partial}{\partial\bbeta}\left(ln\left(1+e^{{\bbeta'{\bm x_i}}}\right)\right)$.  Using the chain rule with $z=\bbeta'{\bm x_i}$
\begin{align}
\frac{\partial}{\partial\bbeta}ln\left(1+e^{{\bbeta'{\bm x_i}}}\right)&=\frac{\partial}{\partial z}ln\left(1+e^{z}\right)\frac{\partial z}{\partial {\bbeta}}\\
&=\frac{e^{z}}{1+e^{z}}{\bm x_i}\\
&=\frac{e^{\bbeta'{\bm x_i}}}{1+e^{\bbeta'{\bm x_i}}}{\bm x_i}
\end{align} 
Putting this together it can be written as
\begin{equation}
\frac{\partial lnL(\bbeta)}{\partial\bbeta}=X'{\bm y}-\sum_{i=1}^{n}\left(
\frac{e^{\bbeta'{\bm x_i}}}{1+e^{\bbeta'{\bm x_i}}}
\right){\bm x_i}
\end{equation}
Note that 
\begin{equation*}
\sum_{i=1}^{n}\left(
\frac{e^{\bbeta'{\bm x_i}}}{1+e^{\bbeta'{\bm x_i}}}
\right){\bm x_i}= \frac{e^{\bbeta'{\bm x_1}}}{1+e^{\bbeta'{\bm x_1}}}{\bm x_1}+
\frac{e^{\bbeta'{\bm x_2}}}{1+e^{\bbeta'{\bm x_2}}}{\bm x_2}+\cdots+
\frac{e^{\bbeta'{\bm x_n}}}{1+e^{\bbeta'{\bm x_n}}}{\bm x_n}
\end{equation*}
Which can be written in matrix vector form as
\begin{equation}
\left({\bm x_1}\,{\bm x_2}\,\cdots\,{\bm x_n}\right)
\left(\begin{array}{c}
\Lambda({\bbeta'{\bm x_1}})\\
\Lambda({\bbeta'{\bm x_2}})\\
\vdots\\
\Lambda({\bbeta'{\bm x_n}})
\end{array}
\right)=X'\blambda
\end{equation}
Putting everything together, the gradient is
\begin{equation}
\frac{\partial lnL(\bbeta)}{\partial\bbeta}=X'\left({\bm y}-\blambda\right)
\end{equation}
\subsection*{Hessian}
The Hessian is given by
\begin{equation}
\frac{\partial^2 lnL(\bbeta)}{\partial\bbeta\partial\bbeta'}=\frac{\partial}{\partial\bbeta'}\left(X'{\bm y}-\sum_{i=1}^{n}\left(
\frac{e^{\bbeta'{\bm x_i}}}{1+e^{\bbeta'{\bm x_i}}}
\right){\bm x_i}\right)
\end{equation}
To evaluate this we need
\begin{equation}
\frac{\partial}{\partial\bbeta'}\left(\frac{e^{\bbeta'{\bm x_i}}}{1+e^{\bbeta'{\bm x_i}}}
\right){\bm x_i}={\bm x_i}\frac{\partial}{\partial\bbeta'}\left(\frac{e^{{\bm x_i}'\bbeta}}{1+e^{{\bm x_i}'\bbeta}}
\right)
\end{equation}
Using the chain rule again
\begin{align}
\frac{\partial}{\partial\bbeta'}\left(\frac{e^{{\bm x_i}'\bbeta}}{1+e^{{\bm x_i}'\bbeta}}
\right)&=\frac{\partial}{\partial z}\left(\frac{e^z}{1+e^{z}}
\right)\frac{\partial z}{\partial\bbeta'}\\
&=\frac{1}{(1+e^{z})^2}\frac{\partial z}{\partial\bbeta'}\\
&=\frac{1}{\left(1+e^{{\bm x_i}'\bbeta}\right)^2}{\bm x_i}'
\end{align}
Putting everything together
\begin{equation}
\frac{\partial^2 lnL(\bbeta)}{\partial\bbeta\partial\bbeta'}=-\sum_{i=1}^{n} \left(1+e^{{\bm x_i}'\bbeta}\right)^{-2}{\bm x_i}{\bm x_i}'
\end{equation}
This can also be written in matrix form:
\begin{align*}
-\sum_{i=1}^{n} \left(1+e^{{\bm x_i}'\bbeta}\right)^{-2}{\bm x_i}{\bm x_i}'&=
-\left(1+e^{{\bm x_1}'\bbeta}\right)^{-2}{\bm x_1}{\bm x_1}'
-\cdots
-\left(1+e^{{\bm x_n}'\bbeta}\right)^{-2}{\bm x_n}{\bm x_n}'\\
&=-\left(
\begin{array}{ccc}
{\bm x_1} & \cdots & {\bm x_n}
\end{array}
\right)
\left(
\begin{array}{ccc}
\left(1+e^{{\bm x_1}'\bbeta}\right)^{-2} & \cdots & 0\\
\vdots & \ddots & \vdots\\
0 & \cdots & \left(1+e^{{\bm x_n}'\bbeta}\right)^{-2}
\end{array}
\right)
\left(
\begin{array}{c}
{\bm x'_1} \\ \vdots\\ {\bm x'_n}
\end{array}
\right)\\
&=-X'WX
\end{align*}
\end{document}
