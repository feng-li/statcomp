{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b2ce5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Newton Algorithm\n",
    "\n",
    "\n",
    "Feng Li\n",
    "\n",
    "School of Statistics and Mathematics\n",
    "\n",
    "Central University of Finance and Economics\n",
    "\n",
    "[feng.li@cufe.edu.cn](mailto:feng.li@cufe.edu.cn)\n",
    "\n",
    "[https://feng.li/statcomp](https://feng.li/statcomp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9538c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding a maximum/minimum\n",
    "\n",
    "-   Suppose we want to find an minimum or maximum of a function $f(x)$\n",
    "\n",
    "-   First order condition: Find the derivative $f'(x)$ and find $x^*$\n",
    "    such that $f'(x^*)=0$\n",
    "\n",
    "-   This is the same as finding a root of the first derivative. We can\n",
    "    use the Newton Raphson algorithm on the first derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d4be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newtonâ€™s algorithm for finding local minima/maxima\n",
    "\n",
    "1.  Select initial value $x_0$ and set $n=0$\n",
    "\n",
    "2.  Set $x_{n+1}=x_{n}-\\frac{f'(x_n)}{f''(x_n)}$\n",
    "\n",
    "3.  Evaluate $|f'(x_{n+1})|$\n",
    "\n",
    "    -   If $|f'(x_{n+1})|<\\epsilon$ then stop.\n",
    "\n",
    "    -   Otherwise set $n=n+1$ and go back to step 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87108",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different Stopping Rules Three stopping rules can be used\n",
    "\n",
    "-   $|f'(x_{n})|\\leq\\epsilon$\n",
    "\n",
    "-   $|x_{n}-x_{n-1}|\\leq\\epsilon$\n",
    "\n",
    "-   $|f(x_{n})-f(x_{n-1})|\\leq\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e97df2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuition\n",
    "\n",
    "-   Focus the **step size** $-\\frac{f'(x)}{f''(x)}$.\n",
    "\n",
    "-   The **signs** of the derivatives control the **direction** of the\n",
    "    next step.\n",
    "\n",
    "-   The **size** of the derivatives control the **size** of the next\n",
    "    step.\n",
    "\n",
    "-   Consider the concave function $f(x)=-x^4$ which has $f'(x)=-4x^3$\n",
    "    and $f''(x)=-12x^2$. There is a maximum at $x^{*}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac1a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Role of first derivative\n",
    "\n",
    "-   If $f''(x)$ is negative the function is locally **concave**, and the\n",
    "    search is for a local **maximum**\n",
    "\n",
    "-   To the left of this maximum $f'(x)>0$\n",
    "\n",
    "-   Therefore $-\\frac{f'(x)}{f''(x)}>0$.\n",
    "\n",
    "-   The next step is to the right.\n",
    "\n",
    "-   The reverse holds if $f'(x)<0$\n",
    "\n",
    "-   Large absolute values of $f'(x)$ imply a steep slope. A big step is\n",
    "    needed to get close to the optimum. The reverse hold for small\n",
    "    absolute value of $f'(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c543f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-   If $f''(x)$ is positive the function is locally **convex**, and the\n",
    "    search is for a local **minimum**\n",
    "\n",
    "-   To the left of this maximum $f'(x)<0$\n",
    "\n",
    "-   Therefore $-\\frac{f'(x)}{f''(x)}>0$.\n",
    "\n",
    "-   The next step is to the right.\n",
    "\n",
    "-   The reverse holds if $f'(x)>0$\n",
    "\n",
    "-   Large absolute values of $f'(x)$ imply a steep slope. A big step is\n",
    "    needed to get close to the optimum. The reverse hold for small\n",
    "    absolute value of $f'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea4130",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Role of second derivative\n",
    "\n",
    "-   Together with the sign of the first derivative, the sign of the\n",
    "    second derivative controls the direction of the next step.\n",
    "\n",
    "-   A larger second derivative (in absolute value) implies a more\n",
    "    curvature\n",
    "\n",
    "-   In this case smaller steps are need to stop the algorithm from\n",
    "    overshooting.\n",
    "\n",
    "-   The opposite holds for a small second derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c4f46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multidimensional Optimization\n",
    "\n",
    "Functions with more than one input\n",
    "\n",
    "-   Most interesting optimization problems involve **multiple** inputs.\n",
    "\n",
    "    -   In determining the most risk efficient portfolio the return is a\n",
    "        function of many weights (one for each asset).\n",
    "\n",
    "    -   In least squares estimation for a linear regression model, the\n",
    "        sum of squares is a function of many coefficients (one for each\n",
    "        regressor).\n",
    "\n",
    "-   How do we optimize for functions $f({\\mathbf{ x}})$ where ${\\mathbf{ x}}$ is a\n",
    "    vector?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c5050",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- First derivative. Simply take the **partial derivatives** and put them in\n",
    "a vector $$\\frac{\\partial y}{\\partial{\\mathbf x}}=\n",
    "    \\left(\n",
    "      \\begin{array}{c}\n",
    "        \\frac{\\partial y}{\\partial x_1}\\\\\n",
    "        \\frac{\\partial y}{\\partial x_2}\\\\\n",
    "        \\vdots\\\\\n",
    "        \\frac{\\partial y}{\\partial x_d}\n",
    "      \\end{array}\n",
    "    \\right)$$ This is called the **gradient** vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6cc757",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- An example. The function $$y=x_1^2-x_1x_2+x_2^2+e^{x_2}$$\n",
    "\n",
    "Has gradient vector $$\\frac{\\partial y}{\\partial{\\mathbf x}}=\n",
    "    \\left(\n",
    "      \\begin{array}{c}\n",
    "        2x_1-x_2\\\\\n",
    "        -x_1+2x_2+e^{x_2}\n",
    "      \\end{array}\n",
    "    \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f19114",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Second derivative. Simply take the second order **partial derivatives**. This will give a matrix\n",
    "$$\\frac{\\partial y}{\\partial{\\mathbf x}\\partial{\\mathbf x}'}=\n",
    "    \\left(\n",
    "      \\begin{array}{cccc}\n",
    "        \\frac{\\partial^2 y}{\\partial x_1^2}&\\frac{\\partial^2 y}{\\partial x_1\\partial x_2}&\\cdots&\\frac{\\partial^2 y}{\\partial x_1\\partial x_d}\\\\\n",
    "        \\frac{\\partial^2 y}{\\partial x_2\\partial x_1}&\\frac{\\partial^2 y}{\\partial x_2^2}&\\cdots&\\frac{\\partial^2 y}{\\partial x_2\\partial x_d}\\\\\n",
    "        \\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "        \\frac{\\partial^2 y}{\\partial x_d\\partial x_1}&\\frac{\\partial^2 y}{\\partial x_d\\partial x_2}&\\cdots&\\frac{\\partial^2 y}{\\partial x_d^2}\\\\\n",
    "      \\end{array}\n",
    "    \\right)$$ This is called the **Hessian** matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dfecc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- An example. The function $$y=x_1^2-x_1x_2+x_2^2+e^{x_2}$$ has Hessian matrix $$\\frac{\\partial y}{\\partial{\\mathbf x}\\partial{\\mathbf x}'}=\n",
    "    \\left(\n",
    "      \\begin{array}{cc}\n",
    "        2 & -1\\\\\n",
    "        -1 & 2 + e^{x_2}\n",
    "      \\end{array}\n",
    "    \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d106e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries for matrix derivatives\n",
    "\n",
    "1.  The derivative of a vector\n",
    "    $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\\\ \\end{bmatrix}$\n",
    "    , by a scalar $x$ is written (in numerator layout notation) as\n",
    "    $$\\begin{aligned}\n",
    "          \\frac{\\partial \\mathbf{y}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x}\\\\ \\frac{\\partial y_2}{\\partial x}\\\\ \\vdots\\\\ \\frac{\\partial y_m}{\\partial x}\\\\ \\end{bmatrix}.\n",
    "        \\end{aligned}$$ In vector calculus the derivative of a vector\n",
    "    $y$ with respect to a scalar $x$ is known as the tangent vector of\n",
    "    the vector $y$, $\\frac{\\partial \\mathbf{y}}{\\partial x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83e081",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.  **The derivative of a scalar $y$ by a vector**\n",
    "    $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix}$\n",
    "    , is written (in numerator layout notation) as $$\\begin{aligned}\n",
    "          \\frac{\\partial y}{\\partial \\mathbf{x}} = \\left[ \\frac{\\partial y}{\\partial x_1} \\ \\ \\frac{\\partial y}{\\partial x_2} \\ \\ \\cdots \\ \\ \\frac{\\partial y}{\\partial x_n} \\right].\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b486bd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.  **The second order derivatives of a scalar $y$ by a vector**\n",
    "    $\\mathbf{x}  = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix}$\n",
    "    is written (in numerator layout notation) as\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "          \\frac{\\partial^2 y}{\\partial \\mathbf{x}\\partial \\mathbf{x}'}&=\\frac{\\partial\n",
    "                                                                        }{\\partial \\mathbf{x}'}\\left[\\frac{\\partial y}{\\partial \\mathbf{x}}\\right]=\\frac{\\partial}{\\partial \\mathbf{x}'}\\left[ \\frac{\\partial y}{\\partial x_1} \\ \\ \\frac{\\partial y}{\\partial x_2} \\ \\ \\cdots \\ \\ \\frac{\\partial y}{\\partial x_n} \\right] \\\\&= \\begin{bmatrix} \\frac{\\partial^2 y}{\\partial x_1^2} & \\frac{\\partial^2 y}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 y}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial^2 y}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 y}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 y}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial^2 y}{\\partial x_m\\partial x_1} & \\frac{\\partial^2 y}{\\partial x_m\\partial x_2} & \\cdots & \\frac{\\partial^2 y}{\\partial x_m\\partial x_m}\\\\ \\end{bmatrix}.\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4deb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "4.  The derivative of a vector function (a vector whose components are\n",
    "    functions)\n",
    "    $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\\\ \\end{bmatrix}$\n",
    "    , with respect to an input vector,\n",
    "    $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix}$\n",
    "    , is written (in numerator layout notation) as\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "          \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{bmatrix}.\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45582e76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5.  The derivative of a matrix function $Y$ by a scalar $x$ is known as\n",
    "    the tangent matrix and is given (in numerator layout notation) by\n",
    "    $$\\begin{aligned}\n",
    "          \\frac{\\partial \\mathbf{Y}}{\\partial x} = \\begin{bmatrix} \\frac{\\partial y_{11}}{\\partial x} & \\frac{\\partial y_{12}}{\\partial x} & \\cdots & \\frac{\\partial y_{1n}}{\\partial x}\\\\ \\frac{\\partial y_{21}}{\\partial x} & \\frac{\\partial y_{22}}{\\partial x} & \\cdots & \\frac{\\partial y_{2n}}{\\partial x}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y_{m1}}{\\partial x} & \\frac{\\partial y_{m2}}{\\partial x} & \\cdots & \\frac{\\partial y_{mn}}{\\partial x}\\\\ \\end{bmatrix}.\n",
    "        \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80718de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6.  The derivative of a scalar $y$ function of a matrix $X$ of\n",
    "    independent variables, with respect to the matrix $X$, is given (in\n",
    "    numerator layout notation) by\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "          \\frac{\\partial y}{\\partial \\mathbf{X}} = \\begin{bmatrix} \\frac{\\partial y}{\\partial x_{11}} & \\frac{\\partial y}{\\partial x_{21}} & \\cdots & \\frac{\\partial y}{\\partial x_{p1}}\\\\ \\frac{\\partial y}{\\partial x_{12}} & \\frac{\\partial y}{\\partial x_{22}} & \\cdots & \\frac{\\partial y}{\\partial x_{p2}}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial y}{\\partial x_{1q}} & \\frac{\\partial y}{\\partial x_{2q}} & \\cdots & \\frac{\\partial y}{\\partial x_{pq}}\\\\ \\end{bmatrix}.\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e91a91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7. Many rules and tricks for matric derivatives are available from \n",
    "\n",
    "    [`Helmut LÃ¼tkepohl (1996), Handbook of Matrices, Chapter 10`](../Literature/Matrix-Derivatives.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b9f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newtonâ€™s algorithm for multidimensional optimization \n",
    "\n",
    "We can now generalise the update step in Newtonâ€™s method:\n",
    "$$x_{n+1}=x_n-\\left(\\frac{\\partial^2 f({\\mathbf x})}{\\partial {\\mathbf x}\\partial{\\mathbf x}'}\\right)^{-1}\n",
    "    \\frac{\\partial f({\\mathbf x})}{\\partial {\\mathbf x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec0aa6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab\n",
    "\n",
    "Write code to minimise $y=x_1^2-x_1x_2+x_2^2+e^{x_2}$\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
