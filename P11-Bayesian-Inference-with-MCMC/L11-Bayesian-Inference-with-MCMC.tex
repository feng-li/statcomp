\documentclass[10pt]{beamer}

%% Chinese support
%% \usepackage[adobefonts,nocap]{ctex}

%% Fonts
\usepackage{multicol}
%\usepackage{mathabx}
\usepackage[scaled]{helvet}
\usepackage{lmodern}
%\usepackage{eulervm}
\usefonttheme[onlymath]{serif}
\usefonttheme{professionalfonts}
\usefonttheme{structurebold}
\usepackage{bm}
\usepackage{verbatim}

%% Color & Theme
\definecolor{SUblue}{RGB}{0,0,180}
\usecolortheme[RGB={0,0,180}]{structure}
\usetheme{Boadilla}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[circle]
\setbeamerfont{title}{size=\large}
\setbeamerfont{frametitle}{size=\large}
\setbeamerfont{framesubtitle}{size=\large,shape =$\color{violet}{\looparrowdownright}~$}
\setbeamercolor{title}{fg=white, bg= SUblue!75!green}
\setbeamercolor{framesubtitle}{fg=violet}
% \setlength{\leftmargini}{5pt}


\title[Statistical Computing]{{\textbf{Bayesian Inference with MCMC}}}

\author[Feng Li]{\includegraphics[height=2cm]{cufelogo}\\
  \vspace{0.5cm}\textbf{Feng Li\\\texttt{feng.li@cufe.edu.cn}}}

\institute[SAM.CUFE.EDU.CN]{\footnotesize{\textbf{School of Statistics and Mathematics\\
      Central University of Finance and Economics}}}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%% Title page
\begin{frame}[plain]
  \titlepage
  \tiny{Revised on \today}
\end{frame}




%% Outline page
\section*{Today we are going to learn...}
\begin{frame}
  \frametitle{Today we are going to learn...}
  \tableofcontents
\end{frame}



\section{Classical v Bayesian}
\begin{frame}{Inference}
  \begin{itemize}
  \item Let's consider a simple problem of inference.

  \item Suppose I am interested in the {\bf proportion} of students in this class who are born in Beijing.  This will be denoted as $\theta$

  \item In particular, I am interested in whether {\bf half} of the class is born in Beijing ($\theta=0.5$) or whether it is less than half ($\theta<0.5$).

  \item I am unable to ask everybody in the class if they were born in Beijing, I can only take a sample.
  \end{itemize}
\end{frame}
\begin{frame}{Classical Framework}
  \begin{itemize}
  \item In the {\bf classical framework} $\theta$ is NOT a random variable.  It is a fixed number that is unknown.

  \item Using the sample, an estimate $\hat{\theta}$ can be obtained.

  \item A 95\% confidence interval around $\hat{\theta}$ can be constructed

  \item The null hypothesis $\theta=0.5$ can be tested against the alternative $\theta< 0.5$
  \end{itemize}
\end{frame}
\begin{frame}{Intepretation in Classical Framework}
  \begin{itemize}
  \item Suppose the 95\% confidence interval is (0.3-0.45).  How is this interpreted?

  \item Correct (classical) interpretation:

    \begin{itemize}
    \item If an infinite number of samples is taken, 95\% of the confidence intervals constructed in this way will contain the true value $\theta$.
    \end{itemize}

  \item Incorrect (classical) interpretation:

    \begin{itemize}

      {
      \item {There is a 95\% probability that $\theta$ is in the interval 0.35-0.45.}
      }
      {
      \item There is a 95\% probability that $\theta$ is in the interval 0.35-0.45.
      }
    \end{itemize}

  \item Reason: $\theta$ is not a random variable.
  \end{itemize}
\end{frame}
\begin{frame}{Intepretation in Classical Framework}
  \begin{itemize}
  \item Suppose $\hat{\theta}=0.4$ and the Null is rejected at the 5\% level of significance.

  \item Correct (classical) interpretation:

    \begin{itemize}
    \item If the null were true than the probability of observing $\hat{\theta}\leq 0.4$ is less than the level of significance (5\%).  Therefore the null is rejected and we conclude  $\hat{\theta}\leq 0.5$
    \end{itemize}

  \item Incorrect (classical) interpretation:

    \begin{itemize}

      {
      \item {The probability that $\theta\leq 0.5$ is 95\%.}
      }
      {
      \item The probability that $\theta\leq 0.5$ is 95\%.
      }
    \end{itemize}

  \item Reason: $\theta$ is not a random variable.
  \end{itemize}
\end{frame}
\begin{frame}{Bayesian Framework}
  \begin{itemize}
  \item There is another way to do statistical inference known as the {\bf Bayesian Framework}.

  \item It relies on a very different understanding of probability.

  \item In the Bayesian framework probability distributions represent uncertainty about quantities that are unknown.

  \item In our example, under the Bayesian framework $\theta$ IS a random variable.
  \end{itemize}
\end{frame}
\begin{frame}{Why Bayes?}
  \begin{itemize}
  \item Let ${\bm y}=(y_1,y_2,\ldots,y_n)'$ be the data in the sample, and $\theta$ be the unknown parameter of interest.

  \item Inferences about $\theta$ are based on the distribution $p(\theta|{\bm y})$.  This can be found using Bayes' Rule

    \begin{equation}
      p(\theta|{\bm y})=\frac{p({\bm y}|\theta)p(\theta)}{p({\bm y})}
    \end{equation}

  \item Consider each term in the equation
  \end{itemize}
\end{frame}
\begin{frame}{Posterior Distribution}
  \begin{itemize}
  \item The term $p(\theta|{\bm y})$ is called the {\bf posterior distribution}.

  \item The word `posterior' comes from a Latin word meaning `after'

  \item This represents our belief about $\theta$ after seeing the data.

  \item All inference is based on this quantity.
  \end{itemize}
\end{frame}
\begin{frame}{Likelihood}
  \begin{itemize}
  \item The term $p({\bm y}|\theta)$ is called the {\bf likelihood}.

  \item This should be familiar since it is the same `likelihood' used in maximum likelihood.

  \item This represents our belief about how the data are generated for a given value of $\theta$.

  \item In the example about students born in Beijing, it will be a Bernoulli distribution.
  \end{itemize}
\end{frame}
\begin{frame}{Normalising Constant}
  \begin{itemize}
  \item The term $p({\bm y})$ can be found using the formula

    \begin{equation}
      p({\bm y})=\int_{\theta}p({\bm y},\theta)d\theta=\int_{\theta}p({\bm y}|\theta)p(\theta)d\theta
    \end{equation}

  \item However, since this term does not contain $\theta$ it forms part of the normalising constant of $p(\theta|{\bm y})$

  \item Sometimes we write Bayes Rule as:

    \begin{equation}
      p(\theta|{\bm y})\propto p({\bm y}|\theta)p(\theta)
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{The Prior}
  \begin{itemize}
  \item The term $p(\theta)$ is called the {\bf prior distribution}.

  \item The word `prior' comes from a Latin word meaning `before'

  \item This represents our belief about $\theta$ before seeing the data.

  \item This is the most controversial part of the Bayesian Framework
  \end{itemize}
\end{frame}
\begin{frame}{Uniform Prior}
  \begin{itemize}
  \item Suppose I know nothing about the proportion of students born in Beijing before I collect data.

  \item In this case $p(\theta)\sim U(0,1)$.

  \item Since the University is in Beijing, I could place less weight on $p(\theta)$ close to 0.

  \item Since Zhong Cai is a good university it attracts students from all of China. I could also place less weight on $p(\theta)$ close to 1.

  \item However, I will use the assumption $p(\theta)\sim U(0,1)$.
  \end{itemize}
\end{frame}
\begin{frame}{Computing the posterior}
  Let $y_i=1$ if the student is born in Beijing and $y_i=0$ otherwise:
  \begin{align}
    p(\theta|{\bm y})&\propto p({\bm y}|\theta)p(\theta)\\
                     &\prod_{i=1}^n \theta^{y_i}(1-\theta)^{(1-y_i)}\times 1\\
                     &\theta^{\sum\nolimits_{i=1}^ny_i}(1-\theta)^{\sum\nolimits_{i=1}^n(1-y_i)}\\
                     &\theta^{\sum\nolimits_{i=1}^ny_i}(1-\theta)^{n-\sum\nolimits_{i=1}^n(y_i)}
  \end{align}
  Does this look familiar?
\end{frame}
\begin{frame}{Normalizing Constant and Kernel}
  What are the
  { normalizing constant}{{\color{blue} normalizing constant}}
  and
  {kernel}{{\color{red} kernel}}
  of the Beta density?
{
    \begin{equation}
      \mbox{Beta}(x;a,b)={\color{blue}\frac{\Gamma(a+b)}{(\Gamma(a)\Gamma(b))}}{\color{red}x^{a-1}(1-x)^{b-1}}
    \end{equation}
  }

\end{frame}

\begin{frame}{Matching}
  \begin{itemize}
  \item The kernel of the $x\sim\mbox{Beta}(a,b)$ is


    {\begin{equation}
        {\color{blue} x}^{{\color{red} a-1}}(1-{\color{blue} x})^{{\color{magenta} b-1}}
        \label{eq:betgen}
      \end{equation}}

  \item The kernel of the posterior $p(\theta|y)$

    \begin{equation}
      {\color{blue} \theta}^{{\color{red}\sum\limits_{i=1}^ny_i}}(1-{\color{blue} \theta})^{{\color{magenta} n-\sum\limits_{i=1}^n(y_i)}}
      \label{eq:betpost}
    \end{equation}


  \item Match $x$ in Equation~\ref{eq:betgen} with $\theta$ in Equation~\ref{eq:betpost}. What about $a$ and $b$?

  \end{itemize}
\end{frame}

\begin{frame}{Posterior}
  \begin{itemize}
  \item Match $a-1$ in Equation~\ref{eq:betgen} with $\sum\limits_{i=1}^ny_i$ in Equation~\ref{eq:betpost}

  \item Match $b-1$ in Equation~\ref{eq:betgen} with $n-\sum\limits_{i=1}^ny_i$ in Equation~\ref{eq:betpost}

  \item The posterior is
    \begin{equation}
      \theta|y\sim\mbox{Beta}\left(\sum\limits_{i=1}^ny_i+1,n-\sum\limits_{i=1}^ny_i+1\right)
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{Inference}
  \begin{itemize}
  \item In classical inference there is {\bf Confidence Interval}

  \item In Bayesian inference there is a similar idea called {\bf Credible Interval}

  \item Find the quantiles at 2.5\% and 97.5\% to form a 95\% credible interval

  \item You can do this in R using {\em qbeta}.

  \item How do we intepret a 95\% credible interval $(L,U)$?

  \item It is just $\mbox{Pr}\left(L\leq\theta\leq U\right)=0.95$
  \end{itemize}
\end{frame}
\begin{frame}{Point Estimates}
  \begin{itemize}
  \item Suppose we want a single estimate for $\theta$.  There
    are a few choices.

    \begin{itemize}

    \item Posterior Median $\theta^*: \int\nolimits_{0}^{\theta^*}p(\theta|{\bm y})d\theta=0.5$

    \item Posterior Mode: $\mathrm{arg max}_\theta ~p(\theta|{\bm y})$

    \item Posterior Mean: $E_{\theta|{\bm y}}[\theta]$
    \end{itemize}

  \item For the Beta distribution, you can look these up in a textbook.

  \item Can you work out the posterior median and posterior mode using R?

  \item Can you approximate the posterior mean using R?
  \end{itemize}
\end{frame}
\begin{frame}{Testing}
  \begin{itemize}
  \item In the Classical framework there is the concept of a Hypothesis test.

  \item For example $H_0:\theta=0.5$ $H_1:\theta<0.5$

  \item In the Bayesian framework just look at posterior probabilities.

  \item For example $\mbox{Pr}(\theta<0.5)$
  \end{itemize}
\end{frame}
\begin{frame}{Classical v Bayesian}
  \begin{itemize}
  \item Many statisticians argue about whether the Classical or Bayesian framework is correct.

  \item Regardless of which one you think is correct, you should understand both.

  \item In particular you should understand how to make interpretations in both frameworks.

  \item The most important distinction is that parameters are NOT random variables in the classical framework but parameters ARE random variables in the Bayesian framework.
  \end{itemize}
\end{frame}
\section{Inference for Gaussian Data}
\begin{frame}{Gaussian Data}
  \begin{itemize}
  \item Suppose we would like to conduct Bayesian inference on the average height of adult males in a country.

  \item The observations are iid $y_i\sim N(\mu,\sigma^2)$.

  \item For now just consider inference on the average height treating the variance of height $\sigma^2$ as known.  We want $p(\mu|y,\sigma^2)$.

  \item What is a reasonable prior on $\mu$?
  \end{itemize}
\end{frame}
\begin{frame}{Prior on $\mu$}
  \begin{itemize}
  \item The prior $p(\mu)$ can also be normally distributed $\mu\sim N(\eta,\tau^2)$

  \item Different sets of values of $\eta$ and $\tau^2$ can be used.

    \begin{itemize}
    \item $p(\mu)\sim N(1.8,0.01)$
    \item $p(\mu)\sim N(1.74,0.0025)$
    \item $p(\mu)\sim N(1.78,0)$
    \item $p(\mu)\sim N(\eta,\tau^2)\quad\tau^2\rightarrow\infty$
    \end{itemize}

  \item These represent prior beliefs before we see any data.
  \item Let the the prior on $\mu$ be independent of $\sigma^2$.  We say  $\mu$ and $\sigma^2$ are  {\bf independent a priori}.
  \end{itemize}
\end{frame}
\begin{frame}{What will be the posterior?}
  \begin{itemize}
  \item The prior is $N(\eta,\tau^2)$

  \item The likelihood is $N(\mu,\sigma^2)$

  \item Maybe the posterior is also normal $N(a,b)$?

  \item What does the kernel of the normal look like?
  \end{itemize}
\end{frame}
\begin{frame}{Kernel of the Normal}
  \begin{align*}
    p(x)&=(2\pi b)^{-1/2}exp\left\{-\frac{1}{2}\left[\frac{(x-a)^2}{b}\right]\right\}\\
        &\propto exp\left\{-\frac{1}{2}\left[\frac{(x-a)^2}{b}\right]\right\}\\
        &\propto exp\left\{-\frac{1}{2}\left[\frac{(x^2-2ax+a^2}{b}\right]\right\}\\
        &\propto exp\left\{-\frac{1}{2}\left[\frac{(x^2-2ax}{b}\right]\right\}
          exp\left\{-\frac{1}{2}\left[\frac{a^2}{b}\right]\right\}\\
        &\propto exp\left\{-\frac{1}{2}\left[\frac{(x^2-2ax}{b}\right]\right\}
  \end{align*}
\end{frame}
\begin{frame}{Obtain Posterior}
  \begin{align*}
    p(\mu|{\bm y},\sigma^2) &\propto p({\bm y}|\mu,\sigma^2)p(\mu|\sigma^2)\\
                            &\propto p({\bm y}|\mu,\sigma^2)p(\mu)\\
                            &\propto \left(\prod\limits_{i=1}^nexp\left\{-\frac{1}{2}\left[\frac{\left(y_i-\mu\right)^2}{\sigma^2}\right]\right\}\right)
                              exp\left\{-\frac{1}{2}\left[\frac{\left(\mu-\eta\right)^2}{\tau^2}\right]\right\}\\
                            &\propto exp\left\{-\frac{1}{2}\left[\frac{\sum\limits_{i=1}^n\left(y_i-\mu\right)^2}{\sigma^2}\right]\right\}exp\left\{-\frac{1}{2}\left[\frac{\left(\mu-\eta\right)^2}{\tau^2}\right]\right\}\\
  \end{align*}
\end{frame}
\begin{frame}{Obtain Posterior}
  \begin{align*}
    &\propto exp\left\{-\frac{1}{2}\left[\frac{\sum\limits_{i=1}^n\left(y_i-\mu\right)^2}{\sigma^2}+\frac{\left(\mu-\eta\right)^2}{\tau^2}\right]\right\}\\
    &\propto exp\left\{-\frac{1}{2}\left[\frac{\sum\limits_{i=1}^n\left(y^2_i-2y_i\mu+\mu^2\right)}{\sigma^2}+\frac{\left(\mu^2-2\eta\mu+\eta^2\right)}{\tau^2}\right]\right\}\\
    &\propto exp\left\{-\frac{1}{2}\left[\frac{\sum\limits_{i=1}^n(y_i^2)-2\mu\sum\limits_{i=1}^n y_i+n\mu^2}{\sigma^2}+\frac{\left(\mu^2-2\eta\mu+\eta^2\right)}{\tau^2}\right]\right\}\\
  \end{align*}
\end{frame}
\begin{frame}{Obtain Posterior}
  \begin{align*}
    &\propto exp\left\{-\frac{1}{2}\left[\left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)\mu^2-2\left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right)\mu
      +\left(\frac{\sum\limits_{i=1}^{n} (y^2_i)}{\sigma^2}+\frac{\eta^2}{\tau^2}\right)\right]\right\}\\
    &\propto exp\left\{-\frac{1}{2}\left[\left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)\mu^2-2\left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right)\mu
      \right]\right\}\\
  \end{align*}
\end{frame}
\begin{frame}{Matching}
  Kernel of Normal


  {
    \begin{equation*}
      exp\left\{-\frac{1}{2}\left[{\color{red} \frac{1}{b}}{\color{blue} x}^2-2{\color{magenta} \frac{a}{b}}{\color{blue} x}\right]\right\}
    \end{equation*}
  }

  Match $x$ to $\mu$.  Then find $a$ and $b$


  {
    \begin{equation*}
      exp \left\{-\frac{1}{2}\left[{\color{red} \left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)}{\color{blue} \mu}^2-2{\color{magenta} \left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right)}{\color{blue} \mu}\right]\right\}
    \end{equation*}
  }

\end{frame}
\begin{frame}{Matching}
  Match coefficient of $\mu^2$ and $x^2$
  \begin{equation}
    {b=\left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)^{-1}}, ~{\frac{1}{b}=\frac{n}{\sigma^2}+\frac{1}{\tau^2}}
  \end{equation}


  Match coefficient of $\mu$ and $x$
  \begin{equation}
    {a=\left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)^{-1}\left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right)},~{\frac{a}{b}=\left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right)}
  \end{equation}

\end{frame}
\begin{frame}{Posterior}
  The posterior is
  \begin{equation}
    \mu|y,\sigma^2\sim N\left(
      \left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)^{-1}\left(\frac{\sum\limits_{i=1}^{n} y_i}{\sigma^2}+\frac{\eta}{\tau^2}\right),
      \left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)^{-1}
    \right)
  \end{equation}
\end{frame}
\begin{frame}{Coding Time}
  Write code to plot the density of the posterior ($N(a,b)$ with a,b defined previously).
  Use the values:
  \begin{itemize}
  \item $\sigma^2=0.01$
  \item $n=10$
  \item $\sum y_i=17.34$
  \item $\eta=1.8$
  \item $\tau^2=0.25$
  \end{itemize}
\end{frame}
\begin{frame}{Tips for code}
  \begin{itemize}
  \item Generate a grid of values using {\em x=seq(1.55,1.85,0.001)}

  \item Write code to work out $a$ and $b$

  \item Evaluate density using {\em y=dnorm(x,a,sqrt(b))}

  \item Plot using {\em plot(x,y,"l")}
  \end{itemize}
\end{frame}
\begin{frame}{Different Values}
  Keep $\sigma^2=0.01$, $n=10$ and $\sum y_i=17.34$.  Change

  \begin{itemize}
  \item $\eta=1.6$
  \item $\tau^2=0.25$
  \end{itemize}

  Now Consider:
  \begin{itemize}
  \item $\eta=1.6$
  \item $\tau^2=0.001$
  \end{itemize}
\end{frame}
\begin{frame}{Plot}
  \begin{center}
    \includegraphics[height=7cm]{./Pics/compare1.pdf}
  \end{center}
\end{frame}
\begin{frame}{Let $\tau^2$ get big}
  \begin{itemize}
  \item In the limiting case where $\tau\rightarrow\infty$, the prior distribution becomes {\bf improper}

  \item In this limiting case $p(\mu)\propto k$ where $k$ is a constant.  This is called a {\bf flat prior}

  \item It can be verified that as $\tau\rightarrow\infty$, the posterior mean $a\rightarrow\sum y_i/n$ and $b\rightarrow \sigma^2/n$.

  \item When $p(\mu)\propto k$ $\mu|{\bm y}, \sigma^2\sim N(\sum y_i/n,\sigma^2/n)$

  \item This leads similar inference as in the classical case ($\sigma^2$ known).
  \end{itemize}
\end{frame}
\begin{frame}{Let $n$ get big}
  Now consider:
  \begin{itemize}
  \item $\sigma^2=0.01$
  \item $n=1000$
  \item $\sum y_i=1734$
  \item $\eta=1.6$
  \item $\tau^2=0.001$
  \end{itemize}
  What do you think happens when $n\rightarrow\infty$?
\end{frame}
\begin{frame}{Plot}
  \begin{center}
    \includegraphics[height=7cm]{./Pics/compare2.pdf}
  \end{center}
\end{frame}
\begin{frame}{Summary}
  \begin{itemize}
  \item If the prior variance is large:

    \begin{itemize}
    \item The posterior mean will be closer to the sample mean.
    \item The posterior variance is larger.
    \end{itemize}

  \item If the prior variance is small:

    \begin{itemize}
    \item The posterior mean will be closer to the prior mean.
    \item The posterior variance is smaller.
    \end{itemize}

  \item As the sample size gets larger, prior information is dominated by the information in the likelihood.
  \end{itemize}
\end{frame}
\begin{frame}{More terminology}
  \begin{itemize}
  \item Improper prior: A prior that does not integrate to 1 for example the {\bf flat prior} $p(\mu)\propto 1$ where $\mu\in\mathbb{R}$

  \item Proper prior: A prior that does integrate to 1.

  \item Noninformative Prior: A prior that has a small effect on on the posterior.  It may be proper or improper

  \item Conjugate Prior: A prior that has the same distribution as the posterior
  \end{itemize}
\end{frame}
\begin{frame}{Examples of Conjugacy}
  \begin{itemize}
  \item Beta prior with Bernoulli likelihood give Beta posterior
    \begin{itemize}
    \item We say ``Beta is conjugate to the Bernoulli''.
    \end{itemize}

  \item Gaussian prior with Gaussian likelihood gives Gaussian posterior
    \begin{itemize}
    \item We say ``Gaussian is conjugate to the Gaussian''.
    \end{itemize}

  \item Gamma prior with Poisson likelihood gives Gamma posterior
    \begin{itemize}
    \item We say ``Gamma is conjugate to the Poisson''.
    \end{itemize}

  \item Not all priors are conjugate.
  \end{itemize}
\end{frame}
\section{Dealing with more than one parameter}
\begin{frame}{What about $\sigma^2$}
  \begin{itemize}
  \item In the previous section we ignored $\sigma^2$.

  \item The posterior we looked at was $p(\mu|\sigma^2,{\bm y})$

  \item In reality we also want to do inference on $\sigma^2$.

  \item Also since we have uncertainty about $\sigma^2$ it doesn't make sense to base inference on $p(\mu|\sigma^2,{\bm y})$
  \end{itemize}
\end{frame}
\begin{frame}{Inference is based on Marginal Posterior}
  \begin{itemize}
  \item Inference for $\mu$ will be based on

    \begin{equation}
      p(\mu|{\bm y})=\int_{\sigma^2}p(\mu,\sigma^2|{\bm y})d\sigma^2
    \end{equation}

  \item Similarly inference for $\sigma^2$ will be based on

    \begin{equation}
      p(\sigma^2|{\bm y})=\int_{\mu^2}p(\mu,\sigma^2|{\bm y})d\mu
    \end{equation}

  \item In both cases
    \begin{equation}
      p(\mu,\sigma^2|{\bm y})\propto p({\bm y}|\mu,\sigma^2)p(\mu,\sigma^2)
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{An interesting integral}
  \begin{itemize}
  \item Another way to write the integral for $p(\mu|{\bm y})$ is as

    \begin{equation*}
      \int_{\sigma^2}p(\mu,\sigma^2|{\bm y})d\sigma^2=
      \int_{\sigma^2}p(\mu|\sigma^2,{\bm y})p(\sigma^2|{\bm y})d\sigma^2
    \end{equation*}

  \item Here is should be  clear that we are `integrating' out or `averaging' out the uncertainty in $\sigma^2$

  \item The `weights' for this average are $p(\sigma^2|{\bm y})$

  \item The same applies to $\int_{\mu}p(\mu,\sigma^2|{\bm y})d\mu$
  \end{itemize}
\end{frame}
\begin{frame}{How do we do the integration?}
  \begin{itemize}
  \item In many cases integration can be done by {\bf recognising} a distribution.

  \item Let's consider the case where $p(\mu,\sigma^2)\propto (\sigma^2)^{-1}$

  \item This is the same as a flat prior on $\mu$ and a flat prior on $log(\sigma^2)$.

  \item We will find $p(\sigma^2|{\bm y})$
  \end{itemize}
\end{frame}
\begin{frame}{Two steps}
  \begin{enumerate}
  \item Recognize a distribution for $\mu$

    \begin{itemize}
    \item To make this integrate to 1 we must have normalizing constant.
    \item Also we cannot remove any terms involving $\sigma^2$
    \end{itemize}

  \item Recognize a distribution in $\sigma^2$
  \end{enumerate}
\end{frame}
\begin{frame}{Find posterior}
  \begin{align*}
    p(\sigma^2|{\bm y})&=\int_{\mu}p({\bm y}|\sigma^2,\mu)p(\sigma^2,\mu)d\mu\\
                       &\propto \int_{\mu}\left(\prod_{i=1}^n (2\pi\sigma^2)^{-1/2}
                         exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]\right)(\sigma^{2})^{-1}d\mu\\
                       &\propto (2\pi\sigma^2)^{-n/2}(\sigma^{2})^{-1}\int_{\mu}\prod_{i=1}^n
                         exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]d\mu\\
                       &\propto (\sigma^2)^{-\frac{n}{2}-1}\int_{\mu}\prod_{i=1}^n
                         exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]d\mu\\
  \end{align*}
\end{frame}
\begin{frame}{The integral}
  Consider the integral
  \begin{equation}
    \int_{\mu}\prod_{i=1}^n
    exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]d\mu
  \end{equation}
  With similar working as before we can show
  \begin{equation*}
    \int_{\mu}\prod_{i=1}^n
    exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]d\mu=
    \int_{\mu}exp\left[-\frac{\sum(y_i^2)-2\mu\sum y_i+n\mu^2}{2\sigma^2}\right]d\mu
  \end{equation*}
\end{frame}
\begin{frame}
  \begin{align*}
    &=\int_{\mu}exp\left[-\frac{-2\mu\sum y_i+n\mu^2}{2\sigma^2}\right]
      exp\left[-\frac{\sum(y_i^2)}{2\sigma^2}\right]d\mu\\
    &=exp\left[-\frac{\sum(y_i^2)}{2\sigma^2}\right]\int_{\mu}exp\left[-\frac{-2\mu\bar{y}+\mu^2}{2(\sigma^2/n)}\right]
      d\mu\\
    &=exp\left[-\frac{\sum(y_i^2)}{2\sigma^2}\right]
      {\color{blue} exp\left[\frac{\bar{y}^2}{2(\sigma^2/n)}\right]}\times\\
    &\quad\quad
      \int_{\mu}exp\left[-\frac{{\color{blue}\bar{y}^2}-2\mu\bar{y}+\mu^2}{2(\sigma^2/n)}\right]
      d\mu\\
  \end{align*}
  where $\bar{y}=\sum y_i/n$
\end{frame}
\begin{frame}
  \begin{align*}
    &=exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]\times\\
    &\quad\quad
      \int_{\mu}exp\left[-\frac{(\mu-\bar{y})^2}{2(\sigma^2/n)}\right]
      d\mu\\
    &=exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]{\color{blue} \times(2\pi)^{1/2}(\sigma^2/n)^{1/2}}\times\\
    &\quad\quad
      \int_{\mu}{\color{blue}(2\pi)^{-1/2}(\sigma^2/n)^{-1/2}}exp\left[-\frac{(\mu-\bar{y})^2}{2(\sigma^2/n)}\right]
      d\mu\\
    &=exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]\times(2\pi)^{1/2}(\sigma^2)^{1/2}n^{-1/2}\\
  \end{align*}
\end{frame}
\begin{frame}{Back to the original aim}
  Now that $\mu$ has been integrated out we can focus on the posterior of $\sigma^2$
  \begin{align*}
    p(\sigma^2|{\bm y})&\propto (\sigma^2)^{-\frac{n}{2}-1}\int_{\mu}\prod_{i=1}^n
                         exp\left[-\frac{(y_i-\mu)^2}{2\sigma^2}\right]d\mu\\
                       &\propto (\sigma^2)^{-\frac{n}{2}-1}exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]\times(2\pi)^{1/2}(\sigma^2)^{1/2}n^{-1/2}\\
                       &\propto (\sigma^2)^{-\frac{(n-1)}{2}-1}exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]\\
  \end{align*}
  Can we recognise this?
\end{frame}
\begin{frame}{Inverse Gamma disribution}
  The kernel of the inverse Gamma disribution is given by
  \begin{equation}
    p(x)\propto x^{-a-1}exp(-b/x)
  \end{equation}
  We have
  \begin{equation}
    p(\sigma^2|{\bm y})\propto (\sigma^2)^{-\frac{(n-1)}{2}-1}exp\left[-\frac{\sum(y_i^2)-n\bar{y}^2}{2\sigma^2}\right]
  \end{equation}
  Match the parameters
\end{frame}
\begin{frame}
  \begin{itemize}
  \item The posterior is

    \begin{equation}
      \sigma^2|{\bm y}\sim IG\left(\frac{n-1}{2},\frac{\sum(y_i^2)-n\bar{y}^2}{2}\right)
    \end{equation}

  \item All inference on the mean is based on this distribution.

  \item A point estimate can be given by $E_{\sigma^2|{\bm y}}[\sigma^2]=\frac{\sum(y_i^2)-n\bar{y}^2}{n-2}$.

  \item Credible intervals for the mean can also be found.

  \item We can do a similar process to show that the posterior $p(\mu|{\bm y})$ follows a Student t distribution.
  \end{itemize}
\end{frame}
\begin{frame}{This is annoying}
  \begin{itemize}
  \item Integrating out the parameters involves tedious mathematics.

  \item Even worse in some cases it does not even lead to a posterior that we can recognize.

  \item For example, for the prior $\mu\sim N(\eta,\tau^2)$, then p($\sigma^2|{\bm y})$ not Inverse Gamma.  It is {\bf unrecognizable}

  \item What can we do?
  \end{itemize}
\end{frame}
\section{MCMC and Bayesian Inference}
\begin{frame}{Markov chain Monte Carlo}
  \begin{itemize}
  \item Although we cannot recognize the posterior, we do know the density.

  \item We may only know the kernel of the density and not the normalizing constant.

  \item Is it possible to at least draw a sample from the joint posterior?
  \end{itemize}
\end{frame}
\begin{frame}{Inference by MCMC}
  \begin{itemize}
  \item Consider the case where $p(\mu,\sigma^2)=p(\mu)p(\sigma^2)$ where

    \begin{itemize}
    \item $\mu\sim N(\eta,\tau^2)$
    \item $p(\sigma^2)\propto (\sigma^2)^{-1}$
    \end{itemize}

  \item In this case the posterior of $\mu$ is:

    \begin{equation}
      p(\mu|{\bm y})\propto
      \left[\sum (y_i-\mu)^2\right]^{-n/2}
      exp\left[-\frac{(\mu-\eta)^2}{2\tau^2}\right]
      \label{eq:tar}
    \end{equation}

  \item This cannot be recognized.
  \end{itemize}
\end{frame}
\begin{frame}{Metropolis Algorithm}
  \begin{itemize}
  \item Use the Metropolis algorithm to simulate a sample from $\mu|{\bm y}$ where the target density is given by Equation~\ref{eq:tar}.

  \item Use the values:
    \begin{itemize}
    \item $n=10$
    \item $\sum y_i=17.34$
    \item $\sum y^2_i=32$
    \item $\eta=1.8$
    \item $\tau^2=0.25$
    \end{itemize}

  \item Make a Monte Carlo approximation of $E[\mu|{\bm y}]$
  \end{itemize}
\end{frame}
\begin{frame}{Summary}
  \begin{itemize}
  \item Bayesian Inference treats all unknown quantities including parameters as {\bf random variables}

  \item All inference is based on the {\bf posterior}

  \item In some cases the posterior can be evaluated and recognized.

  \item In other cases algorithms such as the Metropolis algorithm can be used.

  \item Next week we will look at improvements on the basic Metropolis algorithm.
  \end{itemize}
\end{frame}



\section{Linear Regression with Bayesian Posterior Sampling}

\begin{frame}
  \frametitle{Sampling the Posterior}

  \begin{itemize}
  \item Recall the discussion: why do we need sampling algorithm?
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{The Bayesian Linear Regression model}

  \begin{itemize}
  \item Recall the linear regression model

    \begin{equation*}
      y_i = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p + \epsilon_i
    \end{equation*}
    where $\epsilon_i \sim N(0, \sigma^2)$

  \item We are intertested in the joint distribution of
    $\beta_0,\beta_1,...\beta_p$ and $\sigma^2$

  \item You know how to get that by OLS under the Gaussian assumption:
    \begin{equation*}
      p(\beta,\sigma^2) = p(\beta|\sigma^2)p(\sigma^2)
    \end{equation*}
    where $\beta|\sigma^2 \sim N\left[
      (x'x)^{-1}x'y,(x'x)^{-1}\sigma^2\right]$ and $\sigma^2 \sim \chi^2(n-p)$.

  \item \textbf{The Bayesian approach}:

    \begin{itemize}
    \item We know by the Bayes' rule
      \begin{align*}
        p(\beta,\sigma^2|y,x) & \propto
                                p(y|\beta,\sigma^2,x)p(\beta,\sigma^2) \\
                              & =p(y|\beta,\sigma^2,x)p(\beta|\sigma^2)p(\sigma^2)\\
      \end{align*}
      where $p(y|\beta,\sigma^2,x)$ is the \textbf{likelihood} for the
      model and $p(\beta,\sigma^2) = p(\beta|\sigma^2)p(\sigma^2)$ is
      the \textbf{prior} information of the parameters and
      $p(\beta,\sigma^2|y,x)$ is called the \textbf{posterior}.

    \item \textbf{The Bayesian way: Let's just draw random samples}
      from $p(\beta,\sigma^2|y,x)$.

    \end{itemize}

  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Sampling the Posterior}

  \begin{enumerate}
  \item Write down the likelihood function.
  \item Specify the prior
    \begin{itemize}
    \item The distribution of $\beta$ given $\sigma^2$ that you know a
      priori.
    \item The distribution of $\sigma^2$ (it has to be positive).
    \end{itemize}

  \item Write down the posterior.

  \item Use Gibbs to draw

    Set a initial value for $\beta^{(0)}$ and $\sigma^{2(0)}$.

    \begin{enumerate}
    \item Draw a random vector $\beta^{(1)}$ from $p(\beta^{(1)} | \sigma^{2(0)},y,x)$
    \item Draw a random number $\sigma^{2(1)}$ from $p(\sigma^{2(1)}| \beta^{(1)} ,y,x)$
    \item Draw a random vector $\beta^{(2)}$ from $p(\beta^{(2)} | \sigma^{2(1)},y,x)$
    \item Draw a random number $\sigma^{2(2)}$ from $p(\sigma^{2(2)}| \beta^{(2)} ,y,x)$
    \item Draw a random vector $\beta^{(3)}$ from $p(\beta^{(3)} | \sigma^{2(2)},y,x)$
    \item Draw a random number $\sigma^{(3)}$ from $p(\sigma^{2(3)}| \beta^{(3)} ,y,x)$
    \item ...
    \item ...
    \end{enumerate}

  \item Summarize $\beta^{(1)},\beta^{(2)},...,\beta^{(n)}$
  \item Summarize $\sigma^{(1)},\sigma^{(2)},...,\sigma^{(n)}$

  \item Well done!

  \end{enumerate}

\end{frame}


\begin{frame}
  \frametitle{A real R example}

\end{frame}









\begin{frame}{Using MCMC for Bayesian Inference}
  \begin{itemize}
  \item Today we look at specific examples of using MCMC to do Bayesian inference

  \item We will look at different ways of constructing MCMC for the same model

  \item In particular
    \begin{itemize}
    \item Gibbs Sampler
    \item Method of Composition
    \item Exact Inference
    \end{itemize}
  \end{itemize}
\end{frame}
\begin{frame}{Breaking down further}
  \begin{itemize}
  \item For each step within Method of Composition and Gibbs there may be a few options.

    \begin{itemize}
    \item Generate from a recognized distribution
    \item Generate using Metropolis Hastings with a Laplace approximation as proposal.
    \item Generate using Metropolis Hastings with a random walk proposal (i.e. Metropolis).

    \item Any combination of these
    \end{itemize}

  \item In theory all schemes converge, but some schemes may converge faster than others.

  \item Also each scheme will have different Monte Carlo efficiency and computational efficiency.
  \end{itemize}
\end{frame}
\begin{frame}{Our Example}
  \begin{itemize}
  \item The model is $y_i\sim N(\mu,\sigma^2)$ with prior $p(\mu,\sigma^2)\propto (\sigma^2)^{-1}$

  \item The posterior is
    \begin{equation}
      p(\mu,\sigma^2|{\bm y})\propto
      (\sigma^2)^{-(n/2)-1}
      exp\left\{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^{n}\left(y_i-\mu\right)^2\right\}
    \end{equation}

  \item The algorithms will produce a sample from this posterior
    \begin{equation*}
      \left(\mu^{[1]},{\sigma^2}^{[1]}\right),
      \left(\mu^{[2]},{\sigma^2}^{[2]}\right),
      \ldots,
      \left(\mu^{[M]},{\sigma^2}^{[M]}\right)\sim p(\mu,\sigma^2|{\bm y})
    \end{equation*}

  \item The data is y=c(4.88, 2.71, 5.77, 4.26, 4.10, 2.60, 6.47, 3.76, 2.35, 2.91)
  \end{itemize}
\end{frame}
\begin{frame}{Four densities}
  \begin{itemize}
  \item There are four densities that we will use

  \item Conditional posteriors:

    \begin{itemize}
    \item $p(\mu|\sigma^2,{\bm y})$ (Normal)
    \item $p(\sigma^2|\mu,{\bm y})$ (Inverse Gamma)
    \end{itemize}

  \item Marginal posteriors:

    \begin{itemize}
    \item $p(\mu|{\bm y})$ (Student t)
    \item $p(\sigma^2|{\bm y})$ (Inverse Gamma)
    \end{itemize}

  \item These can be combined in different ways to obtain a sample from the joint posterior $p(\mu,\sigma^2|{\bm y})$
  \end{itemize}
\end{frame}

\begin{frame}{Steps}
  \begin{itemize}
  \item Consider the following steps

    \begin{itemize}
    \item {\bf Obtain} conditional posterior for each parameter.

    \item {\bf Recognize} conditional posterior for each parameter.

    \item {\bf Integrate} to get marginal posterior for each parameter.
    \end{itemize}

  \item Sometimes Step 2 and 3 are impossible.
  \end{itemize}
\end{frame}
\begin{frame}{Four densities}
  \begin{itemize}
  \item In this example, we can recognize/ integrate to get all four densities

  \item Conditional posteriors:

    \begin{itemize}
    \item $p(\mu|\sigma^2,{\bm y})\sim N(\bar{y},\sigma^2/n)$
    \item $p(\sigma^2|\mu,{\bm y})\sim IG(n/2,\sum y_i^2-n\mu^2/2)$
    \end{itemize}

  \item Marginal posteriors:

    \begin{itemize}
    \item $p(\mu|{\bm y})\sim t_{n-1}(\bar{y},s^2/n)$
    \item $p(\sigma^2|{\bm y})\sim IG((n-1)/2,\sum y^2_i-n\bar{y}^2/2)$
    \end{itemize}

  \item Here $\bar{y}=\sum y_i/n$ and
    $s^2=\sum(y_i-\bar{y})^2/(n-1)$
  \end{itemize}
\end{frame}
\section{Gibbs Sampling}
\begin{frame}{Gibbs Sampler}
  \begin{itemize}
  \item The first option is the Gibbs Sampler.

  \item It is the most straightforward.

  \item Inside a single loop:

    \begin{enumerate}
    \item Generate from $p(\sigma^2|\mu,{\bm y})$
    \item Generate from $p(\mu|\sigma^2,{\bm y})$

    \end{enumerate}

  \item The order can be swapped around.
  \end{itemize}
\end{frame}
\begin{frame}{Simulating}
  \begin{itemize}
  \item In this example both distributions are recognisable.  Therefore the Gibbs sampler at step $i$ can be

    \begin{enumerate}
    \item Generate ${\sigma^2}^{[i]}\sim IG(\frac{n}{2},\frac{\sum y_i^2-n{\mu^{[i-1]}}^2}{2})$
    \item Generate $\mu^{[i]}\sim N(\bar{y},{\sigma^2}^{[i]}/n)$

    \end{enumerate}

  \item Always  use the {\bf current values} of the parameters.

  \item If you cannot recognize the distributions then use the Metropolis Hastings algorithm at each step.

  \item This is called Metropolis Hastings within Gibbs.
  \end{itemize}
\end{frame}
\begin{frame}{Coding time}
  \begin{itemize}
  \item Code up the Gibbs sampler on the previous slide.

  \item Note there is no function for generating from an inverse gamma distribution in R (although you can download packages to do this).

  \item Instead use Metropolis Hastings to generate $\sigma^2$ within Gibbs.

  \item You can either use a random walk proposal or a Laplace proposal.  It is up to you.
  \end{itemize}
\end{frame}
%\subsection{Parameter Constraints}
\begin{frame}{Dealing with parameter constraints}
  \begin{itemize}
  \item In this example $\sigma^2>0$ since it is a variance.

  \item In Metropolis Hastings, it is possible to propose a value that ${\sigma^2}^{[new]}<0$

  \item There are a few ways to handle this
    \begin{enumerate}
    \item Reject any ${\sigma^2}^{[new]}<0$
    \item Reparameterise
    \item Constrain proposal
    \end{enumerate}

  \item We will look at the first two in detail
  \end{itemize}
\end{frame}
\begin{frame}{Reject}
  \begin{itemize}
  \item By far the simplest option is to reject any value that does not satisfy parameter constraints.

  \item The rationale is that the target distribution is 0 if the constraints are not satisfied.

  \item Recall the Metropolis Hastings ratio
    \begin{equation}
      \alpha=min\left(1,\frac{p(\theta^{new})q(\theta^{new}\rightarrow\theta^{old})}{p(\theta^{old})q(\theta^{old}\rightarrow\theta^{new})}\right)
    \end{equation}

  \item This is often (but not always) inefficient and leads to low acceptance rates.
  \end{itemize}
\end{frame}
\begin{frame}{Reparameterize}
  \begin{itemize}
  \item Another option is to reparametize.  For example instead of simulating $\sigma^2$ we can simulate $\tau=log\left(\sigma^2\right)$

  \item If we have a sample $(\tau^{[1]},\ldots,\tau^{[M]})$ then the sample from $p(\sigma^2|{\bm y})$ is
    \begin{equation}
      \left({\sigma^2}^{[1]},\ldots,{\sigma^2}^{[M]}\right)=
      \left(exp(\tau)^{[1]},\ldots,exp(\tau)^{[M]}\right)
    \end{equation}

  \item How do we simulate $\tau$?
  \end{itemize}
\end{frame}
\begin{frame}{Reparameterization}
  Let the target density be
  \begin{equation}
    p(\sigma^2)
  \end{equation}

  The density of $\tau=log(\sigma^2)$ is
  \begin{equation}
    p(e^{\tau})J
  \end{equation}

  Where $J=\left|\frac{d\sigma^2}{d\tau}\right|=e^{\tau}$ and is called the {\bf Jacobian} of the transformation
\end{frame}
\begin{frame}{Jacobian}
  Remember densities need to be integrated to find probabilities.  So one way to think of a density is as
  \begin{equation}
    p(\sigma^2)d\sigma^2
  \end{equation}

  Then
  \begin{equation}
    \frac{d\sigma^2}{d\tau}=e^{\tau}
  \end{equation}

  implying
  \begin{equation}
    d\sigma^2=e^{\tau}d\tau
  \end{equation}

  The target density of $\tau$ is
  \begin{equation}
    p(e^{\tau})e^{\tau}d\tau
  \end{equation}
\end{frame}
\begin{frame}{Coding time}
  \begin{itemize}
  \item Try to obtain a sample from $p(\tau|{\bm y})$ using the Metropolis Hastings algorithm.

  \item Wherever you see $\sigma^2$ in the target density in your R Code replace with $exp(\tau)$

  \item Don't forget the Jacobian
  \end{itemize}
\end{frame}
%\subsection{Inference}
\begin{frame}{Inference}
  \begin{itemize}
  \item We have produced a sample from the joint posterior $p(\mu,\sigma^2|{\bm y})$.

  \item We can do joint inference, for example to determine $\mbox{Pr}(\mu<5,\sigma^2>1|{\bm y})$ we only need to count the proportion of our sample where $\mu<5,\sigma^2>1$

  \item We can also do inference on the marginal posterior by simply ignoring the other parameter.

  \item We can do this with no integration!
  \end{itemize}
\end{frame}
\begin{frame}{Monte Carlo Estimate}
  \begin{itemize}
  \item How to find the posterior mean $E(\mu|{\bm y})$?

  \item Use the sample $\mu^{[1]},\ldots,\mu^{[M]}$ ignoring $\sigma^2$

  \item We can find a Monte Carlo estimate

    \begin{equation}
      E(\mu|{\bm y})\approx M^{-1}\sum\limits_{j=1}^M \mu^{[i]}
    \end{equation}

  \item The 95\% credible interval can be found by finding quantiles using the R function {\em quantile}

  \item Try it!
  \end{itemize}
\end{frame}
\section{Method of Composition}
\begin{frame}{Method of Composition}
  \begin{itemize}
  \item An option that is often better than Gibbs is Method of Composition.

  \item Option 1
    \begin{itemize}
    \item Simulate from $p(\sigma^2|{\bm y})$
    \item Simulate from $p(\mu|\sigma^2,{\bm y})$
    \end{itemize}

  \item Option 2
    \begin{itemize}
    \item Simulate from $p(\mu|{\bm y})$
    \item Simulate from $p(\sigma^2|\mu,{\bm y})$
    \end{itemize}

  \item Both will work.  The decision usually comes down to whichever integration is easier.
  \end{itemize}
\end{frame}
\begin{frame}{Option 1}
  \begin{itemize}
  \item To do Option 1, we need
    \begin{equation}
      p(\sigma^2|{\bm y})=\int_\mu p(\mu,\sigma^2|{\bm y})d\mu
    \end{equation}
     and
    \begin{equation}
      p(\mu|\sigma^2,{\bm y})
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{Option 1}
  \begin{itemize}
  \item The algorithm is

    \begin{enumerate}
    \item Simulate ${\sigma^2}^{[i]}\sim
      \mbox{IG}\left(\frac{n-1}{2},\frac{\sum y_i^2-n\bar{y}^2}{2}\right)$
    \item Simulate ${\mu}^{[i]}\sim
      \mbox{N}\left(\bar{y},\frac{{\sigma^2}^{[i]}}{n}\right)$
    \end{enumerate}

  \item We are lucky since both distributions are recognisable.

  \item If they are not recognisable then Metropolis Hastings can be used in either Step 1 or Step 2 or both.
  \end{itemize}
\end{frame}
\begin{frame}{Option 2}
  \begin{itemize}
  \item To do Option 2, we need
    \begin{equation}
      p(\mu|{\bm y})=\int_{\sigma^2} p(\mu,\sigma^2|{\bm y})d\sigma^2
    \end{equation}

    and
    \begin{equation}
      p(\sigma^2|\mu,{\bm y})
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{Option 2}
  \begin{itemize}
  \item The algorithm would be

    \begin{enumerate}
    \item Simulate ${\mu}^{[i]}\sim
      \mbox{t}_{n-1}\left(\bar{y},\frac{s^2}{n}\right)$
    \item Simulate ${\sigma}^{[i]}\sim
      \mbox{IG}\left(\frac{n}{2},\frac{\sum y_i^2-n{\mu^{[i]}}^2}{2}\right)$
    \end{enumerate}

  \item Again both distributions are recognisable.

  \item If they are not recognisable then Metropolis Hastings can be used in either Step 1 or Step 2 or both.
  \end{itemize}
\end{frame}
\begin{frame}{Coding time}
  \begin{itemize}
  \item Obtain a joint sample using Option 1.

  \item Since there is no function to randomly generate from an inverse gamma distribution use Metropolis Hastings to carry out Step 1.

  \item Make sure you deal with the constraint $\sigma^2>0$ correctly.
  \end{itemize}
\end{frame}
\section{Exact Inference}
\begin{frame}{Exact Inference}
  \begin{itemize}
  \item For this example it is not actually necessary to do MCMC.

  \item We already saw
    \begin{equation}
      \sigma^2|{\bm y}\sim \mbox{IG}\left(\frac{n-1}{2},\frac{\sum y_i^2-n\bar{y}^2}{2}\right)
    \end{equation}

  \item We already saw

    \begin{equation}
      \mu|{\bm y}\sim t_{n-1}\left(\bar{y},\frac{s^2}{n}\right)
    \end{equation}
  \end{itemize}
\end{frame}
\begin{frame}{Point Estimates}
  \begin{itemize}
  \item To find the posterior mean $E(\sigma^2|{\bm y})$ we just need to know the expected value of an Inverse Gamma distribution.

  \item The expected value of an $IG(a,b)$ is $b/(a-1)$

  \item In our example

    \begin{equation}
      E(\sigma^2|{\bm y})=\frac{\sum y_i^2-n\bar{y}^2}{n-3}
    \end{equation}

  \item Similarly $E(\mu|{\bm y})=\bar{y}$

  \end{itemize}
\end{frame}
\begin{frame}{Credible Intervals}
  \begin{itemize}
  \item Credible intervals can be found using the inverse CDF of the Inverse Gamma and t distributions.

  \item For $\mu$ this can be found using the R function {\em qt}

  \item It is a little tricky since qt gives the quantiles for a standardised t.
  \end{itemize}
\end{frame}
\begin{frame}{A word of warning}
  \begin{itemize}
  \item In the posterior $p(\mu,\sigma^2|{\bm y})$ $\mu$ and $\sigma^2$ are dependent

  \item However if we sample from both marginal posteriors $p(\mu|{\bm y})$ and $p(\sigma^2|{\bm y})$, the sample values of $\mu^{[j]}$ and ${\sigma^2}^{[j]}$ are independent.

  \item If we simulate from $p(\mu|{\bm y})$ and $p(\sigma^2|{\bm y})$ this does NOT give a sample from the joint posterior

  \item For Gibbs and Method of Composition $\mu^{[j]}$ and ${\sigma^2}^{[j]}$ are {\bf dependent}.
  \end{itemize}
\end{frame}
\begin{frame}{Steps to constructing MCMC algorithms}
  \begin{enumerate}
  \item Write down posterior

    \begin{itemize}
    \item Simply multiply likelihood and prior.  Very Easy
    \end{itemize}

  \item Do I recognize any distributions in the parameters?
    \begin{itemize}
    \item May require some algebra.
    \end{itemize}

  \item Can I integrate out any parameters?
    \begin{itemize}
    \item Be careful with normalizing constants.
    \end{itemize}
  \end{enumerate}
\end{frame}
\begin{frame}{Steps to constructing MCMC algorithms}
  \begin{itemize}
  \item If you can only do Step 1 you can use Metropolis Hastings within Gibbs.

  \item If you can do Step 2 you can use Gibbs but may not need Metropolis Hastings for all the parameters.

  \item If you can do Step 3, you may be able to do Method of Composition.
  \end{itemize}
\end{frame}







\begin{frame}
  \frametitle{Suggested Reading}

  \begin{itemize}
  \item Gelman et all (2014). Bayesian data
    analysis (Vol. 2). Chapman \& Hall/CRC., \textbf{Chapter 3}


    \emph{Monte Carlo Statistical Methods} Book by Christian P Robert and George
    Casella. (2004 edition)

  \end{itemize}

\end{frame}


\end{document}
