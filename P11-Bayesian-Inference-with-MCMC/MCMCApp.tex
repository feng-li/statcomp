\documentclass
%[handout]
{beamer}
\usepackage{bm}
\usetheme{Malmoe}
\begin{document}
\title[MCMC Examples]{Statistical Computing: Examples of MCMC Inference}
\date{June 14}
\author{Anastasios Panagiotelis / Feng Li}
\institute{Monash University / Central University of Finance and Economics}
\maketitle
\begin{frame}{Using MCMC for Bayesian Inference}
\begin{itemize}
\item Today we look at specific examples of using MCMC to do Bayesian inference
\pause
\item We will look at different ways of constructing MCMC for the same model
\pause
\item In particular
\begin{itemize}
\item Gibbs Sampler
\item Method of Composition
\item Exact Inference
\end{itemize}
\end{itemize} 
\end{frame}
\begin{frame}{Breaking down further}
\begin{itemize}
\item For each step within Method of Composition and Gibbs there may be a few options.
\pause
\begin{itemize}
\item Generate from a recognized distribution
\item Generate using Metropolis Hastings with a Laplace approximation as proposal.
\item Generate using Metropolis Hastings with a random walk proposal (i.e. Metropolis).
\pause
\item Any combination of these
\end{itemize}
\pause
\item In theory all schemes converge, but some schemes may converge faster than others.
\pause
\item Also each scheme will have different Monte Carlo efficiency and computational efficiency.
\end{itemize}
\end{frame}
\begin{frame}{Our Example}
\begin{itemize}
\item The model is $y_i\sim N(\mu,\sigma^2)$ with prior $p(\mu,\sigma^2)\propto (\sigma^2)^{-1}$
\pause
\item The posterior is 
\begin{equation}
p(\mu,\sigma^2|{\bm y})\propto 
(\sigma^2)^{-(n/2)-1}
exp\left\{-\frac{1}{2\sigma^2}\sum\limits_{i=1}^{n}\left(y_i-\mu\right)^2\right\}
\end{equation}
\pause
\item The algorithms will produce a sample from this posterior
\begin{equation*}
\left(\mu^{[1]},{\sigma^2}^{[1]}\right),
\left(\mu^{[2]},{\sigma^2}^{[2]}\right),
\ldots,
\left(\mu^{[M]},{\sigma^2}^{[M]}\right)\sim p(\mu,\sigma^2|{\bm y})
\end{equation*} 
\pause
\item The data is y=c(4.88, 2.71, 5.77, 4.26, 4.10, 2.60, 6.47, 3.76, 2.35, 2.91)
\end{itemize}
\end{frame}
\begin{frame}{Four densities}
\begin{itemize}
\item There are four densities that we will use
\pause
\item Conditional posteriors:
\pause
\begin{itemize}
\item $p(\mu|\sigma^2,{\bm y})$ (Normal)
\item $p(\sigma^2|\mu,{\bm y})$ (Inverse Gamma)
\end{itemize}
\pause
\item Marginal posteriors:
\pause
\begin{itemize}
\item $p(\mu|{\bm y})$ (Student t)
\item $p(\sigma^2|{\bm y})$ (Inverse Gamma)
\end{itemize}
\pause
\item These can be combined in different ways to obtain a sample from the joint posterior $p(\mu,\sigma^2|{\bm y})$
\end{itemize}
\end{frame}

\begin{frame}{Steps}
\begin{itemize}
\item Consider the following steps
\pause
\begin{itemize}
\item {\bf Obtain} conditional posterior for each parameter.
\pause
\item {\bf Recognize} conditional posterior for each parameter.
\pause
\item {\bf Integrate} to get marginal posterior for each parameter.
\end{itemize}
\pause
\item Sometimes Step 2 and 3 are impossible.
\end{itemize}
\end{frame}
\begin{frame}{Four densities}
\begin{itemize}
\item In this example, we can recognize/ integrate to get all four densities
\pause
\item Conditional posteriors:
\pause
\begin{itemize}
\item $p(\mu|\sigma^2,{\bm y})\sim N(\bar{y},\sigma^2/n)$ 
\item $p(\sigma^2|\mu,{\bm y})\sim IG(n/2,\sum y_i^2-n\mu^2/2)$ 
\end{itemize}
\pause
\item Marginal posteriors:
\pause
\begin{itemize}
\item $p(\mu|{\bm y})\sim t_{n-1}(\bar{y},s^2/n)$
\item $p(\sigma^2|{\bm y})\sim IG((n-1)/2,\sum y^2_i-n\bar{y}^2/2)$
\end{itemize}
\pause
\item Here $\bar{y}=\sum y_i/n$ and 
$s^2=\sum(y_i-\bar{y})^2/(n-1)$
\end{itemize}
\end{frame}
\section{Gibbs Sampling}
\begin{frame}{Gibbs Sampler}
\begin{itemize}
\item The first option is the Gibbs Sampler.
\pause
\item It is the most straightforward.
\pause
\item Inside a single loop:
\pause
\begin{enumerate}
\item Generate from $p(\sigma^2|\mu,{\bm y})$
\item Generate from $p(\mu|\sigma^2,{\bm y})$

\end{enumerate}
\pause
\item The order can be swapped around.
\end{itemize}
\end{frame}
\begin{frame}{Simulating}
\begin{itemize}
\item In this example both distributions are recognisable.  Therefore the Gibbs sampler at step $i$ can be
\pause
\begin{enumerate}
\item Generate ${\sigma^2}^{[i]}\sim IG(\frac{n}{2},\frac{\sum y_i^2-n{\mu^{[i-1]}}^2}{2})$
\item Generate $\mu^{[i]}\sim N(\bar{y},{\sigma^2}^{[i]}/n)$

\end{enumerate}
\pause
\item Always  use the {\bf current values} of the parameters.
\pause
\item If you cannot recognize the distributions then use the Metropolis Hastings algorithm at each step.
\pause
\item This is called Metropolis Hastings within Gibbs.
\end{itemize}
\end{frame}
\begin{frame}{Coding time}
\begin{itemize}
\item Code up the Gibbs sampler on the previous slide.
\pause
\item Note there is no function for generating from an inverse gamma distribution in R (although you can download packages to do this).
\pause
\item Instead use Metropolis Hastings to generate $\sigma^2$ within Gibbs.
\pause
\item You can either use a random walk proposal or a Laplace proposal.  It is up to you.
\end{itemize}
\end{frame}
\subsection{Parameter Constraints}
\begin{frame}{Dealing with parameter constraints}
\begin{itemize}
\item In this example $\sigma^2>0$ since it is a variance.
\pause
\item In Metropolis Hastings, it is possible to propose a value that ${\sigma^2}^{[new]}<0$
\pause
\item There are a few ways to handle this
\begin{enumerate}
\item Reject any ${\sigma^2}^{[new]}<0$
\item Reparameterise
\item Constrain proposal
\end{enumerate}
\pause
\item We will look at the first two in detail
\end{itemize}
\end{frame}
\begin{frame}{Reject}
\begin{itemize}
\item By far the simplest option is to reject any value that does not satisfy parameter constraints.
\pause
\item The rationale is that the target distribution is 0 if the constraints are not satisfied.
\pause
\item Recall the Metropolis Hastings ratio
\begin{equation} 
\alpha=min\left(1,\frac{p(\theta^{new})q(\theta^{new}\rightarrow\theta^{old})}{p(\theta^{old})q(\theta^{old}\rightarrow\theta^{new})}\right)
\end{equation}
\pause
\item This is often (but not always) inefficient and leads to low acceptance rates.
\end{itemize}
\end{frame}
\begin{frame}{Reparameterize}
\begin{itemize}
\item Another option is to reparametize.  For example instead of simulating $\sigma^2$ we can simulate $\tau=log\left(\sigma^2\right)$
\pause
\item If we have a sample $(\tau^{[1]},\ldots,\tau^{[M]})$ then the sample from $p(\sigma^2|{\bm y})$ is
\begin{equation}
\left({\sigma^2}^{[1]},\ldots,{\sigma^2}^{[M]}\right)=
\left(exp(\tau)^{[1]},\ldots,exp(\tau)^{[M]}\right)
\end{equation}
\pause
\item How do we simulate $\tau$?
\end{itemize}
\end{frame}
\begin{frame}{Reparameterization}
Let the target density be 
\begin{equation}
p(\sigma^2)
\end{equation}  
\pause
The density of $\tau=log(\sigma^2)$ is
\begin{equation}
p(e^{\tau})J
\end{equation}
\pause
Where $J=\left|\frac{d\sigma^2}{d\tau}\right|=e^{\tau}$ and is called the {\bf Jacobian} of the transformation
\end{frame}
\begin{frame}{Jacobian}
Remember densities need to be integrated to find probabilities.  So one way to think of a density is as
\begin{equation}
p(\sigma^2)d\sigma^2
\end{equation}
\pause
Then
\begin{equation}
\frac{d\sigma^2}{d\tau}=e^{\tau}
\end{equation}
\pause
implying
\begin{equation}
d\sigma^2=e^{\tau}d\tau
\end{equation}
\pause
The target density of $\tau$ is
\begin{equation}
p(e^{\tau})e^{\tau}d\tau
\end{equation}
\end{frame}
\begin{frame}{Coding time}
\begin{itemize}
\item Try to obtain a sample from $p(\tau|{\bm y})$ using the Metropolis Hastings algorithm.
\pause
\item Wherever you see $\sigma^2$ in the target density in your R Code replace with $exp(\tau)$
\pause
\item Don't forget the Jacobian
\end{itemize}
\end{frame}
\subsection{Inference}
\begin{frame}{Inference}
\begin{itemize}
\item We have produced a sample from the joint posterior $p(\mu,\sigma^2|{\bm y})$.
\pause
\item We can do joint inference, for example to determine $\mbox{Pr}(\mu<5,\sigma^2>1|{\bm y})$ we only need to count the proportion of our sample where $\mu<5,\sigma^2>1$
\pause
\item We can also do inference on the marginal posterior by simply ignoring the other parameter.
\pause
\item We can do this with no integration!
\end{itemize}
\end{frame}
\begin{frame}{Monte Carlo Estimate}
\begin{itemize}
\item How to find the posterior mean $E(\mu|{\bm y})$?
\pause
\item Use the sample $\mu^{[1]},\ldots,\mu^{[M]}$ ignoring $\sigma^2$
\pause
\item We can find a Monte Carlo estimate
\pause
\begin{equation}
E(\mu|{\bm y})\approx M^{-1}\sum\limits_{j=1}^M \mu^{[i]}
\end{equation}
\pause
\item The 95\% credible interval can be found by finding quantiles using the R function {\em quantile}
\pause
\item Try it!
\end{itemize}
\end{frame}
\section{Method of Composition}
\begin{frame}{Method of Composition}
\begin{itemize}
\item An option that is often better than Gibbs is Method of Composition.
\pause
\item Option 1
\begin{itemize}
\item Simulate from $p(\sigma^2|{\bm y})$
\item Simulate from $p(\mu|\sigma^2,{\bm y})$
\end{itemize} 
\pause
\item Option 2
\begin{itemize}
\item Simulate from $p(\mu|{\bm y})$
\item Simulate from $p(\sigma^2|\mu,{\bm y})$
\end{itemize}
\pause
\item Both will work.  The decision usually comes down to whichever integration is easier. 
\end{itemize}
\end{frame}
\begin{frame}{Option 1}
\begin{itemize}
\item To do Option 1, we need
\begin{equation}
p(\sigma^2|{\bm y})=\int_\mu p(\mu,\sigma^2|{\bm y})d\mu
\end{equation} 
\pause and
\begin{equation}
p(\mu|\sigma^2,{\bm y})
\end{equation} 
\end{itemize} 
\end{frame}
\begin{frame}{Option 1}
\begin{itemize}
\item The algorithm is
\pause
\begin{enumerate}
\item Simulate ${\sigma^2}^{[i]}\sim 
\mbox{IG}\left(\frac{n-1}{2},\frac{\sum y_i^2-n\bar{y}^2}{2}\right)$
\item Simulate ${\mu}^{[i]}\sim 
\mbox{N}\left(\bar{y},\frac{{\sigma^2}^{[i]}}{n}\right)$
\end{enumerate}
\pause
\item We are lucky since both distributions are recognisable.
\pause
\item If they are not recognisable then Metropolis Hastings can be used in either Step 1 or Step 2 or both. 
\end{itemize}
\end{frame}
\begin{frame}{Option 2}
\begin{itemize}
\item To do Option 2, we need
\begin{equation}
p(\mu|{\bm y})=\int_{\sigma^2} p(\mu,\sigma^2|{\bm y})d\sigma^2
\end{equation} 
\pause
and
\begin{equation}
p(\sigma^2|\mu,{\bm y})
\end{equation} 
\end{itemize} 
\end{frame}
\begin{frame}{Option 2}
\begin{itemize}
\item The algorithm would be
\pause
\begin{enumerate}
\item Simulate ${\mu}^{[i]}\sim 
\mbox{t}_{n-1}\left(\bar{y},\frac{s^2}{n}\right)$
\item Simulate ${\sigma}^{[i]}\sim 
\mbox{IG}\left(\frac{n}{2},\frac{\sum y_i^2-n{\mu^{[i]}}^2}{2}\right)$
\end{enumerate}
\pause
\item Again both distributions are recognisable.
\pause
\item If they are not recognisable then Metropolis Hastings can be used in either Step 1 or Step 2 or both. 
\end{itemize}
\end{frame}
\begin{frame}{Coding time}
\begin{itemize}
\item Obtain a joint sample using Option 1.
\pause
\item Since there is no function to randomly generate from an inverse gamma distribution use Metropolis Hastings to carry out Step 1.
\pause
\item Make sure you deal with the constraint $\sigma^2>0$ correctly.
\end{itemize}
\end{frame}
\section{Exact Inference}
\begin{frame}{Exact Inference}
\begin{itemize}
\item For this example it is not actually necessary to do MCMC.
\pause
\item We already saw
\begin{equation}
\sigma^2|{\bm y}\sim \mbox{IG}\left(\frac{n-1}{2},\frac{\sum y_i^2-n\bar{y}^2}{2}\right)
\end{equation}
\pause
\item We already saw
\pause
\begin{equation}
\mu|{\bm y}\sim t_{n-1}\left(\bar{y},\frac{s^2}{n}\right)
\end{equation}
\end{itemize}
\end{frame}
\begin{frame}{Point Estimates}
\begin{itemize}
\item To find the posterior mean $E(\sigma^2|{\bm y})$ we just need to know the expected value of an Inverse Gamma distribution.
\pause
\item The expected value of an $IG(a,b)$ is $b/(a-1)$
\pause
\item In our example 
\pause
\begin{equation}
E(\sigma^2|{\bm y})=\frac{\sum y_i^2-n\bar{y}^2}{n-3}
\end{equation}
\pause
\item Similarly $E(\mu|{\bm y})=\bar{y}$
\pause
\end{itemize}
\end{frame}
\begin{frame}{Credible Intervals}
\begin{itemize}
\item Credible intervals can be found using the inverse CDF of the Inverse Gamma and t distributions.
\pause
\item For $\mu$ this can be found using the R function {\em qt}
\pause
\item It is a little tricky since qt gives the quantiles for a standardised t.
\end{itemize}
\end{frame}
\begin{frame}{A word of warning}
\begin{itemize}
\item In the posterior $p(\mu,\sigma^2|{\bm y})$ $\mu$ and $\sigma^2$ are dependent
\pause
\item However if we sample from both marginal posteriors $p(\mu|{\bm y})$ and $p(\sigma^2|{\bm y})$, the sample values of $\mu^{[j]}$ and ${\sigma^2}^{[j]}$ are independent.
\pause
\item If we simulate from $p(\mu|{\bm y})$ and $p(\sigma^2|{\bm y})$ this does NOT give a sample from the joint posterior
\pause
\item For Gibbs and Method of Composition $\mu^{[j]}$ and ${\sigma^2}^{[j]}$ are {\bf dependent}.
\end{itemize}
\end{frame}
\begin{frame}{Steps to constructing MCMC algorithms}
\begin{enumerate}
\item Write down posterior
\pause
\begin{itemize}
\item Simply multiply likelihood and prior.  Very Easy
\end{itemize}
\pause
\item Do I recognize any distributions in the parameters?
\begin{itemize}
\item May require some algebra.
\end{itemize}
\pause
\item Can I integrate out any parameters?
\begin{itemize}
\item Be careful with normalizing constants.
\end{itemize}
\end{enumerate}
\end{frame}
\begin{frame}{Steps to constructing MCMC algorithms}
\begin{itemize}
\item If you can only do Step 1 you can use Metropolis Hastings within Gibbs.
\pause
\item If you can do Step 2 you can use Gibbs but may not need Metropolis Hastings for all the parameters.
\pause
\item If you can do Step 3, you may be able to do Method of Composition.
\end{itemize}
\end{frame}
\end{document}